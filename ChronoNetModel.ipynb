{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c536b294-5cb1-4650-a79f-27ed609fb6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob #For getting mat files from our directory\n",
    "\n",
    "import torch #For building our model\n",
    "import torch.nn as nn #For building our model\n",
    "import scipy.io as sio #For loading mat files on this case\n",
    "import mne #For EEG data handling\n",
    "import numpy as np #For arrays and mathematical functions on arrays\n",
    "import sklearn #To train our model\n",
    "import torchmetrics #To assess model accuracy\n",
    "from sklearn.model_selection import GroupKFold #Gets GroupKFold Algorithm / Iterator\n",
    "from sklearn.preprocessing import StandardScaler #Gets StandardScalar to standardize our dataset\n",
    "from sklearn.base import TransformerMixin,BaseEstimator #To create our StandardScalar3D class\n",
    "from pytorch_lightning import LightningModule,Trainer #LightningModule to organize our Pytorch code for Traininer, to train our model.\n",
    "from torch.utils.data import TensorDataset,DataLoader #To create a dataset from Tensors (our features and labels); To load our data in batches and iterate over it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cec5064-f3c7-49bb-b85e-984b00f015cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 14, 512])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input=torch.randn(3, 14, 512) #3 is the \"depth\" of the 3d Tensor, 22 is the \"rows / height\" of the 3D Tensor, and 15000 are the \"columns / width\" of the 3D Tensor\n",
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9912ad54-9fa1-4e4d-acc3-9d26c576b6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionBlock(nn.Module): # Creating the class for our inception block\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels = 32, kernel_size = 2, stride = 2, padding = 0) # Defines 1st Convolution\n",
    "        self.conv2 = nn.Conv1d(in_channels, out_channels = 32, kernel_size = 4, stride = 2, padding = 1) # Defines 2nd Convolution\n",
    "        self.conv3 = nn.Conv1d(in_channels, out_channels = 32, kernel_size = 8, stride = 2, padding = 3) # Defines 3rd Convolution\n",
    "        self.relu=nn.ReLU() # Defines the ReLU Activation Function\n",
    "\n",
    "    #Here, xn is the output of the nth layer.\n",
    "    def forward(self,x): #Defining the forward function\n",
    "        x1 = self.relu(self.conv1(x)) #performing 1st conv and outputting x1\n",
    "        x2 = self.relu(self.conv2(x)) #performing 2nd conv and outputting x2\n",
    "        x3 = self.relu(self.conv3(x)) #performing 3rd conv and outputting x3\n",
    "        x = torch.cat((x1,x2,x3), dim = 1) #taking all outputs of convolutions and concatenating them on 1 Dimension\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fc899a5-28d0-44dd-abdd-408bbf67b0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChronoNet(nn.Module):\n",
    "    def __init__(self, channel):\n",
    "        super().__init__()\n",
    "        self.inception_block1=InceptionBlock(channel) # 1st Inception Block\n",
    "        self.inception_block2=InceptionBlock(96) # 2nd Inception Block\n",
    "        self.inception_block3=InceptionBlock(96) # 3rd Inception Block\n",
    "        self.gru1 = nn.GRU(input_size = 96, hidden_size = 32, batch_first = True) # 1st GRU layer\n",
    "        self.gru2 = nn.GRU(input_size = 32, hidden_size = 32, batch_first = True) # 2nd GRU layer\n",
    "        self.gru3 = nn.GRU(input_size = 64, hidden_size = 32, batch_first = True) # 3rd GRU layer\n",
    "        self.gru4 = nn.GRU(input_size = 96, hidden_size = 32, batch_first = True) # 4th GRU layer\n",
    "        self.relu = nn.ReLU() # ReLU Activation Function\n",
    "        self.gru_linear=nn.Linear(in_features = 64, out_features = 1) # Linear Layer for the 4th GRU\n",
    "        self.flatten = nn.Flatten() # Flattening Layer\n",
    "        self.fc1 = nn.Linear(32,1) # Fully Connected Layer / Output Layer.\n",
    "\n",
    "    def forward(self,x): # Defining the feed forward function\n",
    "        x=self.inception_block1(x) # Fed to Inception Block 1\n",
    "        x=self.inception_block2(x) # Fed to Inception Block 2\n",
    "        x=self.inception_block3(x) # Fed to Inception Block 3\n",
    "        x=x.permute(0,2,1) # Permuted for GRU layers\n",
    "        gru_out1,_=self.gru1(x) # Fed into GRU layer 1\n",
    "        gru_out2,_=self.gru2(gru_out1) # Fed into GRU layer 2\n",
    "        gru_out=torch.cat((gru_out1, gru_out2), dim = 2) # Concatenated, defining the skip connection\n",
    "        gru_out3,_=self.gru3(gru_out)  # Fed into GRU layer 3\n",
    "        gru_out = torch.cat((gru_out1, gru_out2, gru_out3), dim = 2) #C Concatenated, defining the next 2 skip connections\n",
    "        gru_out = gru_out.permute(0,2,1) # Permuted for the linear layer\n",
    "        linear_out=self.relu(self.gru_linear(gru_out)) # Fed into the linear layer to reduce dimensionality\n",
    "        linear_out = linear_out.permute(0,2,1) # Permuted for the 4th GRU layer\n",
    "        gru_out4,_=self.gru4(linear_out) # Fed into the 4th GRU Layer\n",
    "        x=self.flatten(gru_out4) # Data is Flattened for Fully Connected Layer\n",
    "        x=self.fc1(x) # Fed into the Fully Connected Layer\n",
    "        return x # Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9234a6dd-0d7e-4c29-99fc-ba776684d9a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0270],\n",
       "        [0.0246],\n",
       "        [0.0260]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ChronoNet(14)\n",
    "out = model(input)\n",
    "out.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca5317b-3e2d-437c-b808-e91526415863",
   "metadata": {},
   "source": [
    "Now we get our data from our directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59928759-1cc4-4434-bad3-420eb09f9a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "IDD = 'Data/Data/CleanData/CleanData_IDD/Rest'\n",
    "TDC = 'Data/Data/CleanData/CleanData_TDC/Rest'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15ad8f7-afcb-422e-a249-e4d93ebd0ab4",
   "metadata": {},
   "source": [
    "And we define our function for converting our Matlab data for a readable MNE format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f2c89da-38d0-4d51-88c3-7633bcd24100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MatMNE(data):\n",
    "    ch_names = ['AF3', 'F7', 'F3', 'FC5', 'T7', 'P7', 'O1', 'O2', 'P8', 'T8', 'FC6', 'F4', 'F8', 'AF4'] #Defining Channel Names\n",
    "    sampling_freq=128\n",
    "    info = mne.create_info(ch_names, sfreq=sampling_freq, ch_types='eeg') #Creating an MNE Info Object\n",
    "    info.set_montage('standard_1020') #Setting our EEG montage\n",
    "    data = mne.io.RawArray(data, info) #Creating a RawArray to make our data readable for MNE\n",
    "    data.filter(l_freq=1, h_freq=30) #Defining a bandpass filter\n",
    "    data.set_eeg_reference() #Setting our reference EEG by taking the average of all channels\n",
    "    epochs = mne.make_fixed_length_epochs(data, duration = 4) #Creating epochs\n",
    "    return epochs.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39cfdbce-eb41-42e0-ad03-a7a872261ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "idd_subject=[] #Creating a list for IDD subject data\n",
    "\n",
    "for idd in glob.glob(IDD+'/*.mat'): #For each .mat file, read it as idd\n",
    "    data = sio.loadmat(idd)['clean_data'] #for each .mat file, idd, get data under the clean_data key in a NumPy array.\n",
    "    data = MatMNE(data) #Passing our data through our MatMNE function and reassigning to 'data'.\n",
    "    idd_subject.append(data) #Appending the patient data into a singular array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "226a6f7d-15c4-4e52-94cc-50619b6b0e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "tdc_subject=[] #Creating a list for IDD subject data\n",
    "\n",
    "for tdc in glob.glob(TDC+'/*.mat'): #For each .mat file, read it as idd\n",
    "    data = sio.loadmat(tdc)['clean_data'] #for each .mat file, idd, get data under the clean_data key in a NumPy array.\n",
    "    data = MatMNE(data) #Passing our data through our MatMNE function and reassigning to 'data'.\n",
    "    tdc_subject.append(data) #Appending the patient data into a singular array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef6015d0-dc09-4c45-9fc6-929f59c4dc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_epochs_labels=[len(i)*[0] for i in tdc_subject] #Labels our data for TDC as '0'\n",
    "patients_epochs_labels=[len(i)*[1] for i in idd_subject] #Labels our data for IDD as '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ebccc9c-ecbb-4327-93bd-58ef3b66c2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 7\n"
     ]
    }
   ],
   "source": [
    "print(len(control_epochs_labels), len(patients_epochs_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09229a2f-9bea-416d-80b4-68fa630c1bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 14 14\n"
     ]
    }
   ],
   "source": [
    "data_list = tdc_subject + idd_subject #Adds the length of our 2 datasets to a singlular variable\n",
    "label_list=control_epochs_labels + patients_epochs_labels #Adds the labels of our 2 datasets to a singular variable \n",
    "groups_list = [[i]*len(j) for i, j in enumerate(data_list)] #Indexes the test subjects / patients\n",
    "print(len(data_list),len(label_list),len(groups_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38418ab0-49c7-41d2-8d6a-b36ce76ffe83",
   "metadata": {},
   "outputs": [],
   "source": [
    "gkf = sklearn.model_selection.GroupKFold()  # Assigning GroupKFold Function to gkf\n",
    "class StandardScaler3D(BaseEstimator,TransformerMixin): #3D data shape of [Batch, Sequence, Channels]\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "            self.scaler.fit(X.reshape(X.shape[0], -1))\n",
    "            return self\n",
    "        \n",
    "    def transform(self,X):\n",
    "        return self.scaler.transform(X.reshape(X.shape[0], -1)).reshape(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2480166f-17ab-481b-81eb-df2356a953a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(420, 512, 14) (420,) (420,)\n"
     ]
    }
   ],
   "source": [
    "data_array= np.concatenate(data_list) \n",
    "label_array=np.concatenate(label_list)\n",
    "group_array =np.concatenate(groups_list) \n",
    "data_array = np.moveaxis(data_array, 1, 2)\n",
    "\n",
    "print(data_array.shape, label_array.shape, group_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "231b052c-722b-4565-885c-a8e4cf891049",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = [] #Creates a list to store our model accuracies accross iterations\n",
    "for train_index, val_index in gkf.split(data_array, label_array, groups=group_array):\n",
    "    # In GroupKFold cross-validation, get unique indices for training (train_index) and validation (val_index) sets.\n",
    "    # The 'groups' variable ensures that there is no data leakage between different subjects/patients.\n",
    "    train_features, train_labels = data_array[train_index], label_array[train_index]\n",
    "    # Gets training features and labels based on the indices obtained at the kth split.\n",
    "    val_features, val_labels = data_array[val_index], label_array[val_index]\n",
    "    # Gets validation features and labels based on the indices obtained at the kth split.\n",
    "    scaler = StandardScaler3D()  # Initializes a StandardScaler instance for feature scaling.\n",
    "    train_features = scaler.fit_transform(train_features) #Fits the data and then transforms (standardizes) it\n",
    "    val_features = scaler.transform(val_features) #Transforms (standardizes) data\n",
    "    train_features = np.moveaxis(train_features,1,2) #Flip Axis to fit into ChronoNet Architecture\n",
    "    val_features = np.moveaxis(val_features,1,2) #Flip Axis to fit into ChronoNet Architecture\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbdc964b-002a-441c-be01-07d8d91a4096",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = torch.Tensor(train_features)\n",
    "val_features = torch.Tensor(val_features)\n",
    "train_labels = torch.Tensor(train_labels)\n",
    "val_labels = torch.Tensor(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da236775-13fe-4438-880d-bfcb8516a1eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 90)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_features),len(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e12018b8-408d-42bb-b1ff-517c7fdf8bc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(330, 330)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_features), len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1381347b-e93f-45b3-932a-fdad96db457f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChronoModel(LightningModule):\n",
    "    def __init__(self):\n",
    "        super(ChronoModel,self).__init__()\n",
    "        self.model=ChronoNet(14)\n",
    "        self.lr=1e-3 ##Defining learning rate of our model. .0001 per step size\n",
    "        self.bs=12 ## Defining the batch size\n",
    "        self.worker=2 ## Defining the # of workers, a parallel process\n",
    "        self.acc=torchmetrics.Accuracy(task='binary') ## For measuring accuracy of our model.\n",
    "        self.criterion = nn.BCEWithLogitsLoss() ## For measuring accuracy of our model based on the final Sigmoid Activation function.\n",
    "        self.train_outputs = [] #To store our outputs from training the model\n",
    "        self.val_outputs = [] # To store our outputs from validating the model\n",
    "        \n",
    "        \n",
    "    def forward(self,x): ## Defining forward function, for feeding our data into the model. \n",
    "        x=self.model(x)\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self): ## Defining our optimizer\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr) #Implementing the Adam optimizer on our model.\n",
    "\n",
    "    def train_dataloader(self): #Loads our training data\n",
    "        dataset = TensorDataset(train_features,train_labels) #Creates a tensor data object from our Tensors representing our training features\n",
    "        dataloader = DataLoader(dataset, batch_size=self.bs,num_workers = self.worker,shuffle=True) #Loads our data using the dataset, batch size, workers (parallel processes). Our data will be shuffled at each Epoch (per shuffle = True) to prevent overfitting.\n",
    "        return dataloader\n",
    "\n",
    "    def training_step(self,batch,batch_idx): #Defining our function for a single training step\n",
    "        signal,label = batch # From each batch, we unpack signal and label data\n",
    "        output=self(signal.float()) # Output of data given a signal\n",
    "        loss=self.criterion(output.flatten(),label.float().flatten()) #Calculating loss using the output and BCEWithLogitsLoss function\n",
    "        acc=self.acc(output.flatten(),label.long().flatten()) #Calculating the accuracy using the output and the Accuracy() function from torchmetrics\n",
    "        self.train_outputs.append({'loss': loss, 'acc': acc}) # To append / add our output onto our training output list\n",
    "        return {'loss':loss,'acc':acc} # Returns our model loss and accuracy\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        acc=torch.stack([x['acc'] for x in self.train_outputs]).mean().detach().cpu().numpy().round(2) # Takes the average accuracy for the outputs and stacks it onto a singular Tensor. Then detaches it from the gpu, converts it into a NumPy array, round it to the nearest hundreth, and passes it onto the cpu\n",
    "        loss=torch.stack([x['loss'] for x in self.train_outputs]).mean().detach().cpu().numpy().round(2) # Takes the average loss for the outputs and stacks it onto a singular Tensor. Then detaches it from the gpu, converts it into a NumPy array, round it to the nearest hundreth, and passes it onto the cpu\n",
    "        self.train_outputs.clear() #To free up memory after each Epoch\n",
    "        print('train acc loss', acc,loss) # Printing our final training accuracy and loss\n",
    "    \n",
    "    def val_dataloader(self): #Loads our validation data\n",
    "        dataset = TensorDataset(val_features,val_labels) #Creates a tensor data object from our Tensors representing our validation features\n",
    "        dataloader = DataLoader(dataset, batch_size=self.bs,num_workers = self.worker,shuffle=True) #Loads our data using the dataset, batch size, workers (parallel processes). Our data will be shuffled at each Epoch (per shuffle = True) to prevent overfitting.\n",
    "        return dataloader\n",
    "\n",
    "    def validation_step(self,batch,batch_idx): #Defining our function for a single step\n",
    "        signal,label = batch # From each batch, we unpack signal and label data\n",
    "        output=self(signal.float()) # Output of data given a signal\n",
    "        loss=self.criterion(output.flatten(),label.float().flatten()) #Calculating loss using the output and BCEWithLogitsLoss function\n",
    "        acc=self.acc(output.flatten(),label.long().flatten()) #Calculating the accuracy using the output and the Accuracy() function from torchmetrics\n",
    "        self.val_outputs.append({'loss': loss, 'acc': acc}) #To append / add our output onto our validation output list.\n",
    "        return {'loss':loss,'acc':acc} # Returns our model loss and accuracy\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        acc=torch.stack([x['acc'] for x in self.val_outputs]).mean().detach().cpu().numpy().round(2) # Takes the average accuracy for the outputs and stacks it onto a singular Tensor. Then detaches it from the gpu, converts it into a NumPy array, round it to the nearest hundreth, and passes it onto the cpu\n",
    "        loss=torch.stack([x['loss'] for x in self.val_outputs]).mean().detach().cpu().numpy().round(2) # Takes the average loss for the outputs and stacks it onto a singular Tensor. Then detaches it from the gpu, converts it into a NumPy array, round it to the nearest hundreth, and passes it onto the cpu\n",
    "        self.val_outputs.clear() # To free up memory after each epoch.\n",
    "        print('val acc loss', acc, loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "755192f5-7951-48b8-a19d-c1553ba40c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ChronoModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7710dee5-25ff-4e17-842e-b8ed76adbb37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/juanvera/miniconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:67: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    }
   ],
   "source": [
    "trainer=Trainer(max_epochs=1) # Setting up to finally train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "63869725-ac36-4289-828f-93b16e1b4aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type              | Params\n",
      "------------------------------------------------\n",
      "0 | model     | ChronoNet         | 133 K \n",
      "1 | acc       | BinaryAccuracy    | 0     \n",
      "2 | criterion | BCEWithLogitsLoss | 0     \n",
      "------------------------------------------------\n",
      "133 K     Trainable params\n",
      "0         Non-trainable params\n",
      "133 K     Total params\n",
      "0.534     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                        | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juanvera/miniconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "/Users/juanvera/miniconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:436: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc loss 0.29 0.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juanvera/miniconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:436: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n",
      "/Users/juanvera/miniconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:293: The number of training batches (28) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94efff76f8f543a9944145dd8e088045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                               | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc loss 0.32 0.71\n",
      "train acc loss 0.57 0.69\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model) #Training our Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
